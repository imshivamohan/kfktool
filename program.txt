from confluent_kafka import Producer
from confluent_kafka.schema_registry import SchemaRegistryClient, Schema
from confluent_kafka.serialization import JSONSerializer
from confluent_kafka.serialization import SerializationContext
import json

topic = 'temp_readings'

config = {
    'bootstrap.servers': 'localhost:9092',
    'security.protocol': 'SASL_SSL',
    'sasl.mechanism': 'PLAIN',
    'sasl.username': '<your_username>',
    'sasl.password': '<your_password>',
    'ssl.ca.location': '/path/to/ca.crt',
    'ssl.certificate.location': '/path/to/client.crt',
    'ssl.key.location': '/path/to/client.key'
}

data = [
    {"name": "hellori", "age": 33},
    {"name": "siva", "age": 44},
    {"name": "omg", "age": 22}
]

sr_config = {
    'url': '<schema.registry.url>',
    'basic.auth.user.info': '<SR_API_KEY>:<SR_API_SECRET>'
}

schema_registry_client = SchemaRegistryClient(sr_config)

# Get the latest schema version from the Schema Registry
latest_schema = schema_registry_client.get_latest_version(topic + '-value')

if latest_schema is None:
    print(f"Failed to fetch the latest schema version for topic {topic}")
    sys.exit(1)

# Create the Schema object using the fetched schema
schema = Schema(latest_schema.schema_str)

json_serializer = JSONSerializer(schema, schema_registry_client)

producer = Producer(config)

for temp in data:
    producer.produce(topic=topic, key=str(temp['name']),
                     value=json_serializer(temp, SerializationContext(topic, value_schema=schema)))
    producer.flush()
